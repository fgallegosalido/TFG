% !TeX root = ../libro.tex
% !TeX encoding = utf8
\chapter{Complejidad Algorítmica del test AKS}

En este primer capítulo nos vamos a centrar en definir los conceptos necesarios para poder desarrollar la complejidad algorítmica del test AKS y poder comprobar que efectivamente tiene complejidad polinómica en el número de dígitos. Esto es, si la entrada del test es $n$, la complejidad deberá ser polinómica en el logaritmo en base 2 de $n$.

\section{Introducción Complejidad Algorítmica}

El primer paso para poder estudiar la complejidad algorítmica del test AKS está en entender qué es la complejidad algorítmica como tal. Para ello usaremos la notación asintótica $O$, $\Omega$ y $\Theta$.\\

Estas tres notaciones nos sirven para dar forma al concepto de crecimiento asintótico de una función.\\

Además, estas notaciones nos van a servir también para dar forma a la idea intuitiva de que el único término necesario en el comportamiento asintótico es aquel que crece más rápido.

\subsection{Notación $O$}

Empezaremos con el concepto intuitivo de que una función domina asintóticamente a otra según la entrada crece. Para ello damos la siguiente definición.

\begin{definicion}
	Sean $f$ y $g$ dos funciones definidas en $\N$, y cuyas imágenes pertenecen a $\R^+$. Diremos que $f$ es de orden $g$, notado como $O(g(n))$, si, y solo si, $\exists k \in \N$ y $\exists C \in \R^+$ tales que se cumple lo siguiente:
	
	$$f(n) \leq Cg(n) \;\;\;\forall n \in \N;\; n \geq k$$
\end{definicion}

Esta definición nos dice que una función domina a otra dada si la primera multiplicada por una constante es mayor que la segunda para toda entrada a partir de cierto punto. Veamos ahora algunos ejemplos:

\begin{ejemplo}
	Probar que $f(n) = 3n^2 + 1$ es $O(n^2)$.\\
	
	Tomando $k=1$ y $C=4$, podemos ver fácilmente usando inducción sobre $n$ que $3n^2 + 1 \leq 4n^2\;\forall n \geq 1$, luego podemos asegurar que $3n^2 + 1 = O(n^2)$.\\
	
	\begin{itemize}
		\item Si $n=1$, entonces $3 \cdot 1^2 + 1 = 4 \leq 4$, luego se cumple el caso inicial.
		\item Suponiendo cierto para $n$, comprobemos para $n + 1$. Entonces $3(n+1)^2 + 1 = 3n^2 + 6^n + 3 + 1 \leq 4n^2 + 6n + 3 \leq 4n^2 + 8n + 4 = 4(n+1)^2$, luego hemos probado lo que queríamos.
	\end{itemize}
\end{ejemplo}

\subsection{Notación $\Omega$}

Intuitivamente, el concepto de la notación $\Omega$ es el opuesto al concepto de la notación $O$. Lo vemos más rápido en la definición.

\begin{definicion}
	Sean $f$ y $g$ dos funciones definidas en $\N$, y cuyas imágenes pertenecen a $\R^+$. Diremos que $f$ es de orden $g$, notado como $\Omega(g(n))$, si, y solo si, $\exists k \in \N$ y $\exists C \in \R^+$ tales que se cumple lo siguiente:
	
	$$f(n) \geq Cg(n) \;\;\;\forall n \in \N;\; n \geq k$$
\end{definicion}

Viendo la definición, es inmediato ver que, dadas dos funciones $f,g:\N \to \R^+$, entonces $f(n) = O(g(n)) \Leftrightarrow f(n) = \Omega(g(n))$. Algunos ejemplos son:

\begin{ejemplo}
	$3^n = \Omega(2^n)$
\end{ejemplo}

\begin{ejemplo}
	$n^3 + 2n + 3 \neq \Omega(n^4)$
\end{ejemplo}

Realmente este concepto es exactamente igual que el anterior, solo que la acotación la hacemos por debajo en vez de por arriba. Pasaremos entonces al concepto siquiente.

\subsection{Notación $\Theta$}

Este concepto no es más que una manera de indicar que dos funciones se acotan asintóticamente, o lo que es lo mismo, que crecen con la misma rapidez. También se le conoce como el ``orden exacto''. Para ser más exactos, esta es la definición.

\begin{definicion}
	Sean $f$ y $g$ dos funciones definidas en $\N$, y cuyas imágenes pertenecen a $\R^+$. Diremos que $f$ es de orden exacto $g$, notado como $\Theta(g(n))$, si, y solo si
	
	$$f(n) = O(g(n))\;\wedge\;f(n) = \Omega(g(n))$$
\end{definicion}

\subsection{Notación $O^\sim$}

Algunas veces es complicado calcular la complejidad exacta, y puede que nos baste simplemente probar que nuestro algoritmo pertenece a una clase que sigue siendo polinómica. Por ello hacemos la siguiente definición:

\begin{definicion}
	Sea $f:\N \to \R^+$ y definimos $O^\sim(f(n)) = O(f(n) \cdot poly(\log(f(n)))$, donde $poly(n)$ es una función polinómica en $n$.
\end{definicion}

Con esta definición, tenemos que $O^\sim(\log^k(n)) = O(\log^k(n) \cdot poly(\log(\log^k(n))) = O(\log^{k+\epsilon}(n))$.\\

Consideramos $\log(n)$ como el logaritmo en base $2$, y $ln$ como el logaritmo natural.

\section{Complejidad AKS}

En esta sección nos encargaremos de analizar la complejidad algorítmica de cada uno de los pasos del test AKS.\\

Al final de esta sección, tomaremos el paso cuya complejidad sea mayor, y comprobaremos que, efectivamente, el algoritmo AKS está dentro de la clase de complejidad polinomial para algoritmos deterministas.

\subsection{Operaciones básicas}

Para hablar de complejidad de algoritmos complejos, es necesario conocer la complejidad de las operaciones más básicas en matemáticas. Las que vamos a usar en este análisis son sobre todo la multiplicación y división de números enteros y la multiplicación de polinomios.\\

Sean entonces pues las siguientes funciones:\\

\begin{itemize}
	\item $M(n)$ la cantidad de pasos que hay que realizar para multiplicar/dividir dos números enteros de tamaño $n$ bits.
	
	\item $P(n, m)$ la cantidad de pasos que realizar para multiplicar dos polinomios de grado $n$ con coeficientes de tamaño $m$ bits.
\end{itemize}

Es bien sabido ya que $O(M(n)) = O(n^2)$ y que $O(P(n, m)) = O(n^2m^2)$ usando los métodos elementales. Para demostrar que el test \textbf{AKS} tiene complejidad polinómica, estas complejidades serán suficientes para la prueba.\\

La complejidad se puede reducir aún más usando algoritmos más sofisticados. Sabemos que $O(M(n)) = O^\sim(n)$ y que $O(P(n, m)) = O^\sim(nm)$ \cite{modern_computer_algebra}. Estas últimas complejidades serán las que consideraremos al calcular eficiencias, aunque las obtenidas por métodos elementales funcionan perfectamente para probar que el algoritmo AKS es polinomial.

\subsection{Pasos del algoritmo AKS}

En esta sección nos vamos a dedicar a estudiar cada paso del algoritmo y calcular las complejidades de cada uno.\\

Es importante destacar que no es necesario calcular las mejores complejidades posibles. En algunos pasos podemos permitirnos perder un poco de eficiencia (siempre manteniendo la polinomialidad de la misma) sin que esto afecte al hecho de que el algoritmo se ejecuta en tiempo polinómico.

\subsubsection{Paso 1: Potencias Perfectas}

El primer paso del algoritmo consiste en comprobar si la entrada es una potencia perfecta, en cuyo caso es evidente que el número es compuesto.\\

Vamos a describir un algoritmo básico, que aunque no es el más óptimo, será suficiente, pues la verdadera complejidad algorítmica se encuentra en el paso 5, donde comprobamos las identidades polinómicas.\\

Sea entonces pues el siguiente algoritmo:

\begin{algorithm}
	\caption{Potencia Perfecta}\label{perfect_power}
	\begin{algorithmic}[1]
		\Procedure{IsPerfectPower}{$n$}\Comment{Comprobar $n = x^y$ con $x,y > 1$}
			\For{cada $k \leq \log(n)$}
				\State $x = \lfloor n^{1/k} \rfloor$
				\If{$n = x^k$}
					\State \Return{True}
				\EndIf
			\EndFor
			\State \Return{False}
		\EndProcedure
	\end{algorithmic}
\end{algorithm}

A pesar de que hay variantes que usan solo aquellos $k$ primos \cite{bach_sorenson_1989}, de manera que la complejidad se reduce a $O^\sim(\log^3(n))$, nosotros no vamos a hacer dicha selección, de modo que nuestro algoritmo tendrá una complejidad de $O(\log^4(n))$, la cual sigue siendo polinómica en el número de cifras, pero es mucho más sencilla de probar.\\

\begin{teorema}
	El Algoritmo 1 presentado anteriormente tiene complejidad $O(\log^4(n))$.
\end{teorema}

\begin{proof}
	Para calcular $\lfloor n^{1/k} \rfloor$ usando una búsqueda binaria, tenemos que elevar las sucesivas aproximaciones a $k$.\\
	
	La operación de elevar tiene un coste de $O(\log^2(n))$ usando el algoritmo de cuadrados repetidos, y como dicha operación la realizamos $O(\log(n))$ veces (debido a la naturaleza de la búsqueda binaria), podemos concluir que cada iteración del algoritmo ocupa $O(\log^3(n))$.\\
	
	Como tenemos que realizar $O(\log(n))$ iteraciones en total, podemos concluir entonces que la complejidad del Algoritmo 1 es $O(\log^4(n))$.
\end{proof}

Como ya dijimos anteriormente, no vamos a intentar buscar un algoritmo mucho más eficiente, pues el tiempo de ejecución de este paso es básicamente nulo comparado con el tiempo de ejecución del quinto paso, que es en el que nos vamos a centrar más a fondo.

\subsubsection{Paso 2: Encontrar el menor $r$ tal que $ord_r(n) > \log^2(n)$}

Para este paso no necesitamos realmente calcular el orden exacto para cada $r$. Nos bastaría simplemente con probar que $n^k \neq 1 \mod(r)$ para todo $k \leq \log^2(n)$, pues si se cumplen todas esas igualdades, podemos asegurar que $ord_r(n) > \log^2(n)$ para ese $r$ en específico.\\

Lo anterior junto con el hecho de que $r \leq max{3, \lceil \log^5(n) \rceil}$,

Teniendo esto en mente, podemos enunciar el teorema.

\begin{teorema}
	El paso 2 del algoritmo AKS tiene un complejidad de $O^\sim(\log^7(n))$.
\end{teorema}

\begin{proof}
	Por <insertar lema 4.3 del paper>, tenemos que $r \leq max\{3, \lceil \log^5(n) \rceil \}$, es decir, solo hay que comprobar $O(\log^5(n))$ valores de $r$ como mucho.\\
	
	Por otro lado, fijado ya $r$, comprobar $ord_r(n) > \log^2(n)$ es equivalente a comprobar que se cumple $n^k \neq 1 \mod(r)$ para todo $k \leq \log^2(n)$. Esto equivale a hacer $O(\log^2(n))$ comprobaciones para cada $r$, ya que para cada igualdad solo realizamos una multiplicación, que al ser módulo $r$, nos queda que la comprobación tenga complejidad $O(\log^2(n)\log(r)) = O^\sim(\log^2(n))$.\\
	
	Por lo tanto, tenemos que hay que comprobar $O(\log^5(n))$ valores de $r$, y comprobar para cada $r$ cuesta $O^\sim(\log^2(n))$, luego la complejidad del paso 2 es $O^\sim(\log^7(n))$.
\end{proof}

Este paso es importante en el sentido de que la complejidad total del algoritmo depende de la cota que podamos obtener para $r$. A menor $r$, menor complejidad algorítmica.

\subsubsection{Paso 3: Comprobar si $1 < (a, n) < n$ para algún $a \leq r$}

Primero tenemos que calcular una eficiencia para el algoritmo de Euclides. Para ello vamos primero a calcular la cantidad de pasos que toma el algoritmos de Euclides.\\

Para ello, primero debemos hacer un par de comprobaciones. Supongamos que queremos calcular $(a, b)$ con $a > b$. Sabemos que $(a, b) = (b, c)$ con $a = nb + c$ para algún $n \in \N$ o, dicho en términos más claros, $c$ es el resto de dividir $a$ entre $b$ ($c = a \% b$). Por otro lado, $(b, c) = (c, d)$ con $b = mc + d$ para algún $m \in \N$.\\

Puesto que $b > c$, es claro entonces que $m \geq 1$, luego $b = mc + d \geq c + d$, y en consecuencia, $a > c + d$. Sumamos estas dos últimas expresiones y obtenemos $a + b > 2(c + d)$.\\

Estas dos observaciones nos dan a entender que, cada dos pasos, el tamaño del problema se reduce a algo menos de la mitad. Con eso en las manos y asumiendo que $a, b$ tienen como mucho $k$ bits, podemos asegurar que el algoritmo da $O(k)$ pasos.\\

Sabiendo esto y que en cada paso debemos realizar una división, llegamos a que la eficiencia del algoritmo para dos números de tamaño $k$ bits es $O(kM(k)) = O^\sim(k^2)$, es decir, $O^\sim(\log^2(n))$ para el número completo.\\

El algoritmo de Euclides debemos aplicarlo $r$ veces, y sabemos que $r = O(\log^5(n))$, luego el tiempo de ejecución de este paso es $O^\sim(\log^7(n))$.

\subsubsection{Paso 4: Comprobar si $n \leq r$}

Este paso es probablemente el más sencillo de todos, pues solo tenemos que hacer una comparación.\\

Como las comparaciones solo requieren comparar los bits, podemos asegurar que este paso tiene complejidad $O(\log(n))$.

\subsubsection{Paso 5: Comprobar identidades polinómicas}

En este paso tenemos un bucle. La complejidad entonces será el tamaño del bucle multiplicado por el tiempo que tarda cada iteración.\\

Primero tenemos que el número de iteraciones es $\lfloor \sqrt{\phi(r)}\log(n) \rfloor$. Como sabemos que $\phi(r) = r-1$ si $r$ es primo y que $r = O(\log^5(n))$, podemos entonces asegurar lo siguiente:

$$O\left(\lfloor \sqrt{\phi(r)}\log(n) \rfloor\right) = O(\sqrt{r}\log(n)) = O(\log^{5/2}(n)\log(n)) = O(\log^{7/2}(n))$$

Teniendo esto claro, ahora tenemos que comprobar lo que nos cuesta cada iteración. Para ello nos basta primero con saber que la exponenciación requiere de $O(\log(n))$ multiplicaciones de polinomios. Aunque tengamos que realizar dos exponenciaciones ($(X^n + a)^n$ y $X^n$), esto no cambia el hecho de que la cantidad de multiplicaciones de polinomios siga siendo $O(\log(n))$.\\

Ahora veamos lo que nos cuesta cada multiplicación de polinomios. Dado que esta exponenciación se hace módulo $(X^r - 1, n)$, sabemos que el grado de los polinomios va a ser $O(r)$ y el tamaño de los coeficientes $O(\log(n))$. Como ya vimos anteriormente, la multiplicación de polinomios con grado $r$ y coeficientes de tamaño $O(\log(n))$ bits tiene complejidad $O(P(r)M(\log(n)) = O^\sim(r\log(n)) = O^\sim(\log^6(n))$.\\

Con todas estas piezas, tenemos que la complejidad del paso 5 es $O^\sim(\log^{7/2}(n)\log(n)\log^6(n)) = O^\sim(\log^{21/2}(n))$.

\subsection{Resultado final}

Habiendo comprobado las eficiencias de todos los pasos, y sabiendo que el paso 6 no afecta en absoluto, tenemos que las eficiencias de cada paso son:

\begin{enumerate}
	\item $O(\log^4(n))$
	\item $O^\sim(\log^7(n))$
	\item $O^\sim(\log^7(n))$
	\item $O(\log(n))$
	\item $O^\sim(\log^{21/2}(n))$
\end{enumerate}

Puesto que la complejidad del quinto paso es superior a la de los 4 anteriores, podemos concluir con que la complejidad del algoritmo \textbf{AKS} es $O^\sim(\log^{21/2}(n))$.\\

Cabe destacar que, si hubiésemos considerado que $O(M(n)) = O(n^2)$ y que $O(P(n, m)) = O(n^2m^2)$, la complejidad del quinto paso sería $O(\log^{33/2}(n))$, la cual es mucho peor que la obtenida, pero sigue siendo polinómica.

\subsection{Cotas del algoritmo}

En el análisis recién realizado, cabe destacar que la eficiencia del quinto paso (y la del algoritmo en general) depende del valor $r$ encontrado en el segundo paso. Dicho valor sabemos que está acotado superiormente por $max\{\lceil 3, \log^5(n) \rceil\}$, y es lo que nos ha proporcionado la complejidad $O^\sim(\log^{21/2}(n))$.\\

Como sabemos que dicho valor tiene que ser mayor que $\log^2(n) + 1$ por lo explicado anteriormente, podemos entonces asegurar que $r \in \{\log^2(n) + 2, \log^5(n)\}$. De esta manera podemos acotar la complejidad exacta del algoritmo, teniendo así que $O^\sim(\log^{21/2}(n))$ y $\Omega^\sim(\log^6(n))$, y concluyendo que $\Theta(\log^x(n))$ con $x \in [6, 21/2]$.

\endinput
%------------------------------------------------------------------------------------
% FIN DEL CAPÍTULO. 
%------------------------------------------------------------------------------------

% !TeX root = ../libro.tex
% !TeX encoding = utf8
\chapter{Complejidad Algorítmica del test AKS}

En este capítulo nos encargaremos de analizar la complejidad algorítmica de cada uno de los pasos del test AKS, la cual queremos ver que es polinómica en la cantidad de dígitos de la entrada o, lo que es lo mismo, que es logarítmica.\\

Al final de esta sección, tomaremos el paso cuya complejidad sea mayor, y comprobaremos que, efectivamente, el algoritmo AKS está dentro de la clase de complejidad polinomial para algoritmos deterministas.

\section{Operaciones básicas}

Para hablar de complejidad de algoritmos complejos, es necesario conocer la complejidad de las operaciones más básicas en matemáticas. Las que vamos a usar en este análisis son sobre todo la multiplicación y división de números enteros y la multiplicación de polinomios.\\

Sean entonces pues las siguientes funciones:\\

\begin{itemize}
	\item $M(n)$ la cantidad de pasos que hay que realizar para multiplicar/dividir dos números enteros de tamaño $n$ bits.
	
	\item $P(n, m)$ la cantidad de pasos que realizar para multiplicar dos polinomios de grado $n$ con coeficientes de tamaño $m$ bits.
\end{itemize}

Es bien sabido ya que $O(M(n)) = O(n^2)$ y que $O(P(n, m)) = O(n^2m^2)$ usando los métodos elementales. Para demostrar que el test \textbf{AKS} tiene complejidad polinómica, estas complejidades serán suficientes para la prueba.\\

La complejidad se puede reducir aún más usando algoritmos más sofisticados. Sabemos que $O(M(n)) = O^\sim(n)$ y que $O(P(n, m)) = O^\sim(nm)$ \cite{modern_computer_algebra}. Estas últimas complejidades serán las que consideraremos al calcular eficiencias, aunque las obtenidas por métodos elementales funcionan perfectamente para probar que el algoritmo AKS es polinomial.

\section{Pasos del algoritmo AKS}

En esta sección nos vamos a dedicar a estudiar cada paso del algoritmo y calcular las complejidades de cada uno.\\

Es importante destacar que no es necesario calcular las mejores complejidades posibles. En algunos pasos podemos permitirnos perder un poco de eficiencia (siempre manteniendo la polinomialidad de la misma) sin que esto afecte al hecho de que el algoritmo se ejecuta en tiempo polinómico.

\subsection{Paso 1: Potencias Perfectas}

El primer paso del algoritmo consiste en comprobar si la entrada es una potencia perfecta, en cuyo caso es evidente que el número es compuesto.\\

Vamos a describir un algoritmo básico, que aunque no es el más óptimo, será suficiente, pues la verdadera complejidad algorítmica se encuentra en el paso 5, donde comprobamos las identidades polinómicas.\\

Sea entonces pues el siguiente algoritmo:

\begin{algorithm}
	\caption{Potencia Perfecta}\label{perfect_power}
	\begin{algorithmic}[1]
		\Procedure{IsPerfectPower}{$n$}\Comment{Comprobar $n = x^y$ con $x,y > 1$}
			\For{cada $k \leq \log(n)$}
				\State $x = \lfloor n^{1/k} \rfloor$
				\If{$n = x^k$}
					\State \Return{True}
				\EndIf
			\EndFor
			\State \Return{False}
		\EndProcedure
	\end{algorithmic}
\end{algorithm}

A pesar de que hay variantes que usan solo aquellos $k$ primos \cite{bach_sorenson_1989}, de manera que la complejidad se reduce a $O^\sim(\log^3(n))$, nosotros no vamos a hacer dicha selección, de modo que nuestro algoritmo tendrá una complejidad de $O(\log^4(n))$, la cual sigue siendo polinómica en el número de cifras, pero es mucho más sencilla de probar.\\

\begin{teorema}
	El Algoritmo 1 presentado anteriormente tiene complejidad $O(\log^4(n))$.
\end{teorema}

\begin{proof}
	Para calcular $\lfloor n^{1/k} \rfloor$ usando una búsqueda binaria, tenemos que elevar las sucesivas aproximaciones a $k$.\\
	
	La operación de elevar tiene un coste de $O(\log^2(n))$ usando el algoritmo de cuadrados repetidos, y como dicha operación la realizamos $O(\log(n))$ veces (debido a la naturaleza de la búsqueda binaria), podemos concluir que cada iteración del algoritmo ocupa $O(\log^3(n))$.\\
	
	Como tenemos que realizar $O(\log(n))$ iteraciones en total, podemos concluir entonces que la complejidad del Algoritmo 1 es $O(\log^4(n))$.
\end{proof}

Como ya dijimos anteriormente, no vamos a intentar buscar un algoritmo mucho más eficiente, pues el tiempo de ejecución de este paso es básicamente nulo comparado con el tiempo de ejecución del quinto paso, que es en el que nos vamos a centrar más a fondo.

\subsection{Paso 2: Encontrar el menor $r$ tal que $ord_r(n) > \log^2(n)$}

Para este paso no necesitamos realmente calcular el orden exacto para cada $r$. Nos bastaría simplemente con probar que $n^k \neq 1 \mod(r)$ para todo $k \leq \log^2(n)$, pues si se cumplen todas esas igualdades, podemos asegurar que $ord_r(n) > \log^2(n)$ para ese $r$ en específico.\\

Lo anterior junto con el hecho de que $r \leq max{3, \lceil \log^5(n) \rceil}$,

Teniendo esto en mente, podemos enunciar el teorema.

\begin{teorema}
	El paso 2 del algoritmo AKS tiene un complejidad de $O^\sim(\log^7(n))$.
\end{teorema}

\begin{proof}
	Por <insertar lema 4.3 del paper>, tenemos que $r \leq max\{3, \lceil \log^5(n) \rceil \}$, es decir, solo hay que comprobar $O(\log^5(n))$ valores de $r$ como mucho.\\
	
	Por otro lado, fijado ya $r$, comprobar $ord_r(n) > \log^2(n)$ es equivalente a comprobar que se cumple $n^k \neq 1 \mod(r)$ para todo $k \leq \log^2(n)$. Esto equivale a hacer $O(\log^2(n))$ comprobaciones para cada $r$, ya que para cada igualdad solo realizamos una multiplicación, que al ser módulo $r$, nos queda que la comprobación tenga complejidad $O(\log^2(n)\log(r)) = O^\sim(\log^2(n))$.\\
	
	Por lo tanto, tenemos que hay que comprobar $O(\log^5(n))$ valores de $r$, y comprobar para cada $r$ cuesta $O^\sim(\log^2(n))$, luego la complejidad del paso 2 es $O^\sim(\log^7(n))$.
\end{proof}

Este paso es importante en el sentido de que la complejidad total del algoritmo depende de la cota que podamos obtener para $r$. A menor $r$, menor complejidad algorítmica.

\subsection{Paso 3: Comprobar si $1 < (a, n) < n$ para algún $a \leq r$}

Primero tenemos que calcular una eficiencia para el algoritmo de Euclides. Para ello vamos primero a calcular la cantidad de pasos que toma el algoritmos de Euclides.\\

Para ello, primero debemos hacer un par de comprobaciones. Supongamos que queremos calcular $(a, b)$ con $a > b$. Sabemos que $(a, b) = (b, c)$ con $a = nb + c$ para algún $n \in \N$ o, dicho en términos más claros, $c$ es el resto de dividir $a$ entre $b$ ($c = a \% b$). Por otro lado, $(b, c) = (c, d)$ con $b = mc + d$ para algún $m \in \N$.\\

Puesto que $b > c$, es claro entonces que $m \geq 1$, luego $b = mc + d \geq c + d$, y en consecuencia, $a > c + d$. Sumamos estas dos últimas expresiones y obtenemos $a + b > 2(c + d)$.\\

Estas dos observaciones nos dan a entender que, cada dos pasos, el tamaño del problema se reduce a algo menos de la mitad. Con eso en las manos y asumiendo que $a, b$ tienen como mucho $k$ bits, podemos asegurar que el algoritmo da $O(k)$ pasos.\\

Sabiendo esto y que en cada paso debemos realizar una división, llegamos a que la eficiencia del algoritmo para dos números de tamaño $k$ bits es $O(kM(k)) = O^\sim(k^2)$, es decir, $O^\sim(\log^2(n))$ para el número completo.\\

El algoritmo de Euclides debemos aplicarlo $r$ veces, y sabemos que $r = O(\log^5(n))$, luego el tiempo de ejecución de este paso es $O^\sim(\log^7(n))$.

\subsection{Paso 4: Comprobar si $n \leq r$}

Este paso es probablemente el más sencillo de todos, pues solo tenemos que hacer una comparación.\\

Como las comparaciones solo requieren comparar los bits, podemos asegurar que este paso tiene complejidad $O(\log(n))$.

\subsection{Paso 5: Comprobar identidades polinómicas}

En este paso tenemos un bucle. La complejidad entonces será el tamaño del bucle multiplicado por el tiempo que tarda cada iteración.\\

Primero tenemos que el número de iteraciones es $\lfloor \sqrt{\phi(r)}\log(n) \rfloor$. Como sabemos que $\phi(r) = r-1$ si $r$ es primo y que $r = O(\log^5(n))$, podemos entonces asegurar lo siguiente:

$$O\left(\lfloor \sqrt{\phi(r)}\log(n) \rfloor\right) = O(\sqrt{r}\log(n)) = O(\log^{5/2}(n)\log(n)) = O(\log^{7/2}(n))$$

Teniendo esto claro, ahora tenemos que comprobar lo que nos cuesta cada iteración. Para ello nos basta primero con saber que la exponenciación requiere de $O(\log(n))$ multiplicaciones de polinomios. Aunque tengamos que realizar dos exponenciaciones ($(X^n + a)^n$ y $X^n$), esto no cambia el hecho de que la cantidad de multiplicaciones de polinomios siga siendo $O(\log(n))$.\\

Ahora veamos lo que nos cuesta cada multiplicación de polinomios. Dado que esta exponenciación se hace módulo $(X^r - 1, n)$, sabemos que el grado de los polinomios va a ser $O(r)$ y el tamaño de los coeficientes $O(\log(n))$. Como ya vimos anteriormente, la multiplicación de polinomios con grado $r$ y coeficientes de tamaño $O(\log(n))$ bits tiene complejidad $O(P(r)M(\log(n)) = O^\sim(r\log(n)) = O^\sim(\log^6(n))$.\\

Con todas estas piezas, tenemos que la complejidad del paso 5 es $O^\sim(\log^{7/2}(n)\log(n)\log^6(n)) = O^\sim(\log^{21/2}(n))$.

\section{Resultado final}

Habiendo comprobado las eficiencias de todos los pasos, y sabiendo que el paso 6 no afecta en absoluto, tenemos que las eficiencias de cada paso son:

\begin{enumerate}
	\item $O(\log^4(n))$
	\item $O^\sim(\log^7(n))$
	\item $O^\sim(\log^7(n))$
	\item $O(\log(n))$
	\item $O^\sim(\log^{21/2}(n))$
\end{enumerate}

Puesto que la complejidad del quinto paso es superior a la de los 4 anteriores, podemos concluir con que la complejidad del algoritmo \textbf{AKS} es $O^\sim(\log^{21/2}(n))$.\\

Cabe destacar que, si hubiésemos considerado que $O(M(n)) = O(n^2)$ y que $O(P(n, m)) = O(n^2m^2)$, la complejidad del quinto paso sería $O(\log^{33/2}(n))$, la cual es mucho peor que la obtenida, pero sigue siendo polinómica.

\section{Cotas del algoritmo}

En el análisis recién realizado, cabe destacar que la eficiencia del quinto paso (y la del algoritmo en general) depende del valor $r$ encontrado en el segundo paso. Dicho valor sabemos que está acotado superiormente por $max\{\lceil 3, \log^5(n) \rceil\}$, y es lo que nos ha proporcionado la complejidad $O^\sim(\log^{21/2}(n))$.\\

Como sabemos que dicho valor tiene que ser mayor que $\log^2(n) + 1$ por lo explicado anteriormente, podemos entonces asegurar que $r \in \{\log^2(n) + 2, \log^5(n)\}$. De esta manera podemos acotar la complejidad exacta del algoritmo, teniendo así que $O^\sim(\log^{21/2}(n))$ y $\Omega^\sim(\log^6(n))$, y concluyendo que $\Theta(\log^x(n))$ con $x \in [6, 21/2]$.

\endinput
%------------------------------------------------------------------------------------
% FIN DEL CAPÍTULO. 
%------------------------------------------------------------------------------------
